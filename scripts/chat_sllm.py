import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from fuzzywuzzy import process  # ğŸ”¹ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# âœ… ê²½ë¡œ ì„¤ì •
MODEL_PATH = r"C:\Workspace\Chat_Origin\models\finetuned_model"
DATA_PATH = r"C:\Workspace\Chat_Origin\data\data.txt"

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
print("ğŸ”¹ ëª¨ë¸ ë¡œë“œ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)

# âœ… íŠ¹ìˆ˜ í† í° ì¶”ê°€ (ë°˜ë“œì‹œ ëª¨ë¸ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)
special_tokens = {"additional_special_tokens": ["<|startoftext|>", "<|sep|>", "<|endoftext|>"]}
tokenizer.add_special_tokens(special_tokens)
model.resize_token_embeddings(len(tokenizer))

device = torch.device("cpu")
model.to(device).eval()


# âœ… data.txt íŒŒì¼ ë¡œë“œ ë° ë¶„ì„
def load_data():
    """ data.txt íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ì‚¬ì „ í˜•íƒœë¡œ ì €ì¥ """
    if not os.path.exists(DATA_PATH):
        print("âš ï¸ data.txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì°¸ê³  ë°ì´í„° ì—†ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        return {}

    with open(DATA_PATH, "r", encoding="utf-8") as f:
        lines = f.readlines()

    data_dict = {}
    for line in lines:
        if ":" in line:  # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ êµ¬ë¶„ (ì˜ˆ: "ì§ˆë¬¸: ë‹µë³€")
            parts = line.split(":", 1)
            question, answer = parts[0].strip(), parts[1].strip()
            data_dict[question] = answer

    print(f"âœ… {len(data_dict)}ê°œì˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
    return data_dict


# âœ… ë°ì´í„° ë¡œë“œ
data_dict = load_data()


def find_best_match(user_input):
    """ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸ì„ data.txtì—ì„œ ì°¾ì•„ì„œ ë°˜í™˜ """
    if not data_dict:
        return None  # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤

    best_match, score = process.extractOne(user_input, data_dict.keys())

    if score > 75:  # ğŸ”¹ ìœ ì‚¬ë„ 75% ì´ìƒì´ë©´ ì°¸ê³  ë‹µë³€ìœ¼ë¡œ ì œê³µ
        return data_dict[best_match]

    return None


def clean_response(response):
    """ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ í† í° ì œê±°"""
    response = response.replace("<|startoftext|>", "").replace("<|sep|>", "").replace("<|endoftext|>", "")
    response = response.replace("<|endoftext|>", "").strip()
    return response


def chat():
    print("ğŸ’¬ í•œêµ­ì–´ ì±—ë´‡ ì‹¤í–‰ ì¤‘... ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥")

    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            break

        # âœ… data.txtì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸ ì°¾ê¸°
        reference_answer = find_best_match(user_input)

        # âœ… ëª¨ë¸ ì…ë ¥ ìƒì„±
        input_text = f"<|startoftext|>{user_input}<|sep|>"
        input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

        with torch.no_grad():
            output = model.generate(
                input_ids,
                max_length=50,
                do_sample=True,
                top_k=50,
                top_p=0.92,
                temperature=0.7
            )

        response = tokenizer.decode(output[0], skip_special_tokens=True)
        response = clean_response(response)

        # âœ… ì°¸ê³ í•  ë‹µë³€ì´ ìˆìœ¼ë©´ ê°™ì´ ì œê³µ
        if reference_answer:
            print(f"ğŸ”¹ ì°¸ê³  ë‹µë³€: {reference_answer}")
            print(f"ğŸ¤– ëª¨ë¸ ë‹µë³€: {response}")
        else:
            print(f"ğŸ¤– Bot: {response}")


if __name__ == "__main__":
    chat()
