import os
import json

from torch._functorch._aot_autograd.logging_utils import model_name
from transformers import AutoTokenizer, AutoModelForCausalLM

# âœ… ì ˆëŒ€ê²½ë¡œ ì„¤ì •
DATA_DIR = r"C:\Workspace\Chat_Origin\data"
RAW_FILE = os.path.join(DATA_DIR, "dataset.json")  # ì›ë³¸ ë°ì´í„°
PREPROCESSED_PREFIX = os.path.join(DATA_DIR, "preprocessed_dataset")  # ë¶„í•  ì €ì¥ ê²½ë¡œ

# âœ… ëª¨ë¸ ì„ íƒ
MODEL_NAME = "skt/kogpt2-base-v2"
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„° í™•ì¸
for name, param in model.named_parameters():
    print(f"íŒŒë¼ë¯¸í„° ì´ë¦„: {name}, í¬ê¸°: {param.shape}")

# âœ… íŒ¨ë”© í† í° ê°•ì œ ì„¤ì •
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({"pad_token": "[PAD]"})
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.add_special_tokens({"additional_special_tokens": ["<|startoftext|>", "<|sep|>", "<|endoftext|>"]})
    tokenizer.pad_token_id = tokenizer.eos_token_id

# âœ… ìµœëŒ€ í† í° ê¸¸ì´ ë° ë¶„í•  í¬ê¸° ì„¤ì •
MAX_LENGTH = 128
SPLIT_SIZE = 30 * 1024 * 1024  # ğŸš€ 30MB ë‹¨ìœ„ë¡œ íŒŒì¼ì„ ë‚˜ëˆ”

def preprocess_data():
    """ë°ì´í„°ì…‹ì„ í† í°í™”í•˜ì—¬ ë³€í™˜ í›„ ì—¬ëŸ¬ ê°œì˜ JSON íŒŒì¼ë¡œ ë‚˜ëˆ  ì €ì¥"""
    print("ğŸ”„ í•œêµ­ì–´ ì±—ë´‡ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

    # âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ
    with open(RAW_FILE, "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    processed_data = []
    file_index = 1
    current_size = 0

    for example in raw_data:
        question = example["question"].strip()
        answer = example["answer"].strip()

        # âœ… ìƒˆë¡­ê²Œ ë°ì´í„° í¬ë§· ì„¤ì •
        formatted_text = f"<|startoftext|>{question}<|sep|>{answer}<|endoftext|>"

        # âœ… í† í° ë³€í™˜
        tokens = tokenizer(
            formatted_text,
            padding="max_length",
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors="pt"
        )

        data_entry = {
            "input_ids": tokens["input_ids"][0].tolist(),
            "attention_mask": tokens["attention_mask"][0].tolist()
        }

        processed_data.append(data_entry)
        current_size += len(json.dumps(data_entry, ensure_ascii=False).encode("utf-8"))

        # âœ… 30MBë¥¼ ë„˜ìœ¼ë©´ ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥
        if current_size >= SPLIT_SIZE:
            output_file = f"{PREPROCESSED_PREFIX}_{file_index}.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=4)

            print(f"âœ… {output_file} ì €ì¥ ì™„ë£Œ ({len(processed_data)}ê°œ ìƒ˜í”Œ)")
            processed_data = []
            current_size = 0
            file_index += 1

    # âœ… ë‚¨ì€ ë°ì´í„° ì €ì¥ (ë§ˆì§€ë§‰ íŒŒì¼)
    if processed_data:
        output_file = f"{PREPROCESSED_PREFIX}_{file_index}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(processed_data, f, ensure_ascii=False, indent=4)

        print(f"âœ… {output_file} ì €ì¥ ì™„ë£Œ ({len(processed_data)}ê°œ ìƒ˜í”Œ)")

    print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")

if __name__ == "__main__":
    preprocess_data()
