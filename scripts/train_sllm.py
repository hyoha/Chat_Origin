import os
import json
import torch
import torch.multiprocessing as mp
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, get_scheduler
from tqdm import tqdm

# âœ… Windows `multiprocessing` ì„¤ì •
if __name__ == "__main__":
    mp.set_start_method('spawn', force=True)  # Windows í™˜ê²½ì—ì„œ multiprocessing ì˜¤ë¥˜ ë°©ì§€

    # âœ… ê²½ë¡œ ì„¤ì •
    MODEL_NAME = "skt/kogpt2-base-v2"
    MODEL_SAVE_PATH = r"C:\Workspace\Chat_Origin\models\finetuned_model"
    DATA_DIR = r"C:\Workspace\Chat_Origin\data"
    DATA_PREFIX = os.path.join(DATA_DIR, "preprocessed_dataset")

    # âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"ğŸ”¹ ëª¨ë¸ ë¡œë“œ ì¤‘: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # âœ… íŠ¹ìˆ˜ í† í° ì¶”ê°€
    special_tokens = {"additional_special_tokens": ["<|startoftext|>", "<|sep|>", "<|endoftext|>"]}
    tokenizer.add_special_tokens(special_tokens)

    # âœ… ëª¨ë¸ ë¡œë“œ í›„ í† í° í¬ê¸° ì¡°ì •
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
    model.resize_token_embeddings(len(tokenizer))  # âœ… ìƒˆë¡œìš´ í† í°ì„ ë°˜ì˜í•˜ì—¬ ëª¨ë¸ ì¬ì¡°ì •

    # âœ… ëª¨ë¸ì˜ ë‹¨ì–´ ì‚¬ì „ í¬ê¸° í™•ì¸
    vocab_size = model.config.vocab_size
    print(f"ğŸ”¹ ëª¨ë¸ vocab_size: {vocab_size}")

    # âœ… ë°ì´í„° ë¡œë“œ (ì—¬ëŸ¬ ê°œì˜ ë¶„í• ëœ ë°ì´í„° íŒŒì¼ ìë™ ë¡œë“œ)
    print("ğŸ”„ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì¤‘...")
    dataset = []
    file_index = 1

    while True:
        data_file = f"{DATA_PREFIX}_{file_index}.json"
        if not os.path.exists(data_file):
            break

        with open(data_file, "r", encoding="utf-8") as f:
            dataset.extend(json.load(f))

        print(f"âœ… {data_file} ë¡œë“œ ì™„ë£Œ ({len(dataset)}ê°œ ìƒ˜í”Œ)")
        file_index += 1

    # âœ… ë°ì´í„° ê°œìˆ˜ í™•ì¸
    print(f"ğŸ”¹ ì´ í•™ìŠµ ìƒ˜í”Œ ê°œìˆ˜: {len(dataset)}ê°œ")


    # âœ… PyTorch Dataset ë³€í™˜
    class CustomDataset(Dataset):
        def __init__(self, data, vocab_size):
            self.data = data
            self.vocab_size = vocab_size

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            item = self.data[idx]

            # âœ… input_ids ê°’ì´ vocab_sizeë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ í•„í„°ë§
            input_ids = [min(token, self.vocab_size - 1) for token in item["input_ids"]]

            return {
                "input_ids": torch.tensor(input_ids, dtype=torch.long),
                "attention_mask": torch.tensor(item["attention_mask"], dtype=torch.long),
                "labels": torch.tensor(input_ids, dtype=torch.long)
            }


    train_dataset = CustomDataset(dataset, vocab_size)

    # âœ… Windows ìµœì í™” ì„¤ì • (ë©€í‹°í”„ë¡œì„¸ì‹± ì´ìŠˆ í•´ê²°)
    BATCH_SIZE = 2  # âœ… ë°°ì¹˜ í¬ê¸° ì¦ê°€
    EPOCHS = 3  # âœ… í•™ìŠµ íšŸìˆ˜ ì¦ê°€
    GRADIENT_ACCUMULATION_STEPS = 4  # âœ… ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™”
    LEARNING_RATE = 2e-4  # âœ… CPU í•™ìŠµ ìµœì í™”
    NUM_WORKERS = 0  # âœ… Windowsì—ì„œëŠ” 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œ ë°©ì§€

    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True,
        num_workers=NUM_WORKERS, persistent_workers=False
    )

    # âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
    lr_scheduler = get_scheduler(
        "linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=len(train_loader) * EPOCHS,
    )

    # âœ… í•™ìŠµ í™˜ê²½ ì„¤ì • (CPU ì‚¬ìš©)
    device = torch.device("cpu")
    model.to(device)

    # âœ… í•™ìŠµ ë£¨í”„ ì‹¤í–‰
    print("ğŸ”¹ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0

        with tqdm(total=len(train_loader), desc=f"ğŸš€ Epoch {epoch + 1}/{EPOCHS}") as pbar:
            for step, batch in enumerate(train_loader):
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)

                optimizer.zero_grad()
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss
                loss.backward()

                if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:
                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()

                total_loss += loss.item()
                pbar.update(1)
                pbar.set_postfix(loss=loss.item())

        print(f"âœ… Epoch {epoch + 1}/{EPOCHS} ì™„ë£Œ - í‰ê·  Loss: {total_loss / len(train_loader):.4f}")

    # âœ… ëª¨ë¸ ì €ì¥
    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)
    model.save_pretrained(MODEL_SAVE_PATH)
    tokenizer.save_pretrained(MODEL_SAVE_PATH)

    print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ & ì €ì¥ë¨: {MODEL_SAVE_PATH}")
